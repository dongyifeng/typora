[TOC]

# 熵：enthropy

熵度量事物的不确定性，越不确定的事物，它的熵越大。

$Ent(X) = -\sum_{i=1}^{n}{p_i log_2 p_i}$

- n：X 的 n 种不同的离散值

- $p_i$：X 取值为 i 的概率

$Ent(D) $ 的值越小，D 的纯度越高。



例子：假设 X 有两个取值A，B。P(A) =0.5，P(B) =0.5。

​			那么 X 具有的不确定性：$Ent(X)=-(0.5*log_2{0.5}+0.5*log_2{0.5})=log_2{2}=1$

​	

​			假设 X 有两个取值A，B。P(A) =1/3，P(B) =2/3。

​			那么 X 具有的不确定性：$Ent(X)=-(\frac{1}{3}*log_2{\frac{1}{3}}+\frac{2}{3}*log_2{\frac{2}{3}})$

$=\frac{1}{3}log_23-\frac{2}{3}log_2\frac{2}{3}$

$=log_23-\frac{2}{3}log_23-\frac{2}{3}log_2{\frac{2}{3}}$

$=log_23-\frac{2}{3}(log_23+log_2{\frac{2}{3}})$

$=log_23-\frac{2}{3}log_22$

$=log_23-\frac{2}{3}<log_22=1$						



P(A) =1/3，P(B) =2/3  比 P(A) =1/2，P(B) =1/2 确定性大。



**熵只与 X 的分布有关，与 X 取值无关**

```python
    def calc_shannon_ent(data_set):
        label_count = {}
        for feat_vec in data_set:
            label = feat_vec[0]
            label_count[label] = label_count.get(label, 0) + 1

        shannon_ent = 0.0
        n = len(data_set)
        for label, count in label_count.items():
            prob = float(count) / n
            shannon_ent -= prob * math.log(prob, 2)
        return shannon_ent
```



# 联合熵

多个变量的联合熵

$H(X,Y) = -\sum_{x_i \in X}\sum_{y_i \in Y}{p(x_i,y_i)log_2{p(x_i,y_i)}}$



# 条件熵

条件熵类似条件概率，它度量了已知 X 后，剩下 Y 的不确定性。

$H(X|Y) = -\sum_{x_i \in X}\sum_{y_i \in Y}{p(x_i,y_i)log_2{p(x_i|y_i)}}=\sum_{j=1}^np(y_i)H(X|y_i)$



# 信息增益

H(X) 度量了 X 的不确定性。

H(X|Y) 度量了，已知 Y 后 X 剩下的不确定性

H(X) - H(X|Y) 什么能度量什么？

韦恩图



- H(X)：左边的椭圆

- H(Y)：左边的椭圆

- I (X ,Y)互信息或者信息增益：两个椭圆交集

- H( X , Y)：两个椭圆并集

- H(X|Y)

- H(Y|X)

  

$Gain(X,Y) = H(X) - H(X|Y)$

​                     $=-\sum_{i=1}^n{p_i*log_2p_i}-\sum_{j=1}^np(y_i)H(X|y_i)$

例子：15 样本集 D，标签为 [0 , 1]，有 9 条样本为 1，6 条样本为 0 。样本中有特征$A \in\{A_1,A_2,A_3\}$，

- $A=A_1$ 的样本中有 3 个样本的标签为 1，2 个样本的标签为 0
- $A=A_2$ 的样本中有 2 个样本的标签为 1，3 个样本的标签为 0
- $A=A_3$ 的样本中有 4 个样本的标签为 1，1 个样本的标签为 0

样本 D 的熵：$H(D)=-(\frac{9}{15}log_2\frac{9}{15}+\frac{6}{15}log_2\frac{6}{15})=0.971$

样本 D 在特征 A 下的条件熵为：

$H(D|A)=p(A_1)H(D|A_1)+p(A_2)H(D|A_2)+p(A_2)H(D|A_3)$

​               $=\frac{5}{15}H(D|A_1)+\frac{5}{15}H(D|A_2)+\frac{5}{15}H(D|A_3)$

​               $=\frac{5}{15}H(D_1)+\frac{5}{15}H(D_2)+\frac{5}{15}H(D_3)$

​			 $=-\frac{5}{15}(\frac{2}{5}log_2\frac{2}{5}+\frac{3}{5}log_2\frac{3}{5})-\frac{5}{15}(\frac{2}{5}log_2\frac{2}{5}+\frac{3}{5}log_2\frac{3}{5})-\frac{5}{15}(\frac{4}{5}log_2\frac{4}{5}+\frac{1}{5}log_2\frac{1}{5})=0.888$

信息增益：I(D , A) = H(D) - H(D|A)= 0.971 - 0.888 =0.083



# 增益率

为了克服信息增益：对可取值数目较多的属性有偏好。使用增益率。

Gain_ratio(D,a) = $\frac{Gain(D,a)}{IV(a)}$

$IV(a) = -\sum_{i=1}^{n}{\frac{|D_i|}{|D|}log{\frac{|D_i|}{|D|}}}$  模仿信息熵。

- n 是特征 A 取值的个数。

<font color=red>**增益率特点：对可取值数目较少的属性有偏好。**</font>

```python
    def choose_best_feature(self, data_set):
        num_features = len(data_set[0]) - 1
        base_ent = self.calc_shannon_ent(data_set)
        best_info_gain_ratio = 0.0
        best_feature = -1

        for i in range(num_features):
            feat_set = set(example[i] for example in data_set)
            new_ent = 0.0
            split_info = 0.0
            for value in feat_set:
                sub_data_set = self.split_data_set(data_set, i, value)
                prod = len(sub_data_set) / float(len(data_set))
                new_ent += prod * self.calc_shannon_ent(sub_data_set)
                split_info += -prod * math.log2(prod)
            info_gain = base_ent - new_ent
            if info_gain == 0: continue

            info_gain_ratio = info_gain / split_info
            if info_gain_ratio > best_info_gain_ratio:
                best_info_gain_ratio = info_gain_ratio
                best_feature = i

        return best_feature
```



# 基尼指数

基尼值



$Gini(D) =\sum_{k=1}^K{p_k*（1-p_k)}=1-\sum_{k=1}^Kp_k^2$



 <font color=red>Gini(D) 越小，则数据集 D 的纯度越高。</font>

基尼指数：

$Gini\_index(D,a) = \sum_{i=1}^n{\frac{|D_i|}{|D|}Gini(D_i)}$

<font color=red>在属性集合 A 中，选择那个使得划分后基尼指数最小的特征，作为最优化划分特征。</font>