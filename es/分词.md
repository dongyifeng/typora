[TOC]

# ES 内置的分词器

- Standard Analyzer：默认分词器，按词切分，小写处理。
- Simple Analyzer：按照非字母切分（符号被过滤），小写处理。
- Stop Analyzer：小写处理，停用词过滤。
- Whitespace Analyzer：按照空格切分，不转小写。
- Keyword Analyzer：不分词，直接将输入当做输出。
- Patter Analyzer：正则表达式，默认 \W+ （非字符分割）
- Language：提供30多种常见语言的分词器。
- Customer Analyzer：自定义分词器。



# 中文分词



| IK     | 介绍                               | 特点                       | 地址                                                |
| ------ | ---------------------------------- | -------------------------- | --------------------------------------------------- |
| IK     | 实现中英文单词切分                 | 自定义词库                 | http://githup.com/model/elasticsesearch-analysis-ik |
| Hanlp  | 支持分词和词性标注                 | 支持繁体，自定义，并行分词 |                                                     |
| Hanlp  | 有一系列模型算法组成的 java 工具包 |                            |                                                     |
| THULAC | 清华大学中文词法分析工具           |                            |                                                     |



# IK 分词

安装：

bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.1.0/elasticsearch-analysis-ik-7.1.0.zip

演示：ik_smart：最粗粒度的拆分：适合 Phrase 查询。

```json
POST _analyze
{
  "analyzer": "ik_smart",
  "text":"中华人民共和国国歌"
}
```

结果：

```json
{
  "tokens" : [
    {
      "token" : "中华人民共和国",
      "start_offset" : 0,
      "end_offset" : 7,
      "type" : "CN_WORD",
      "position" : 0
    },
    {
      "token" : "国歌",
      "start_offset" : 7,
      "end_offset" : 9,
      "type" : "CN_WORD",
      "position" : 1
    }
  ]
}
```

测试：ik_max_word：会将文本做最细粒度的拆分，适合 Term Query。

```json
POST _analyze
{
  "analyzer": "ik_max_word",
  "text": "中华人民共和国国歌"
}
```



结果：

```json
{
  "tokens" : [
    {
      "token" : "中华人民共和国",
      "start_offset" : 0,
      "end_offset" : 7,
      "type" : "CN_WORD",
      "position" : 0
    },
    {
      "token" : "中华人民",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "CN_WORD",
      "position" : 1
    },
    {
      "token" : "中华",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "CN_WORD",
      "position" : 2
    },
    {
      "token" : "华人",
      "start_offset" : 1,
      "end_offset" : 3,
      "type" : "CN_WORD",
      "position" : 3
    },
    {
      "token" : "人民共和国",
      "start_offset" : 2,
      "end_offset" : 7,
      "type" : "CN_WORD",
      "position" : 4
    },
    {
      "token" : "人民",
      "start_offset" : 2,
      "end_offset" : 4,
      "type" : "CN_WORD",
      "position" : 5
    },
    {
      "token" : "共和国",
      "start_offset" : 4,
      "end_offset" : 7,
      "type" : "CN_WORD",
      "position" : 6
    },
    {
      "token" : "共和",
      "start_offset" : 4,
      "end_offset" : 6,
      "type" : "CN_WORD",
      "position" : 7
    },
    {
      "token" : "国",
      "start_offset" : 6,
      "end_offset" : 7,
      "type" : "CN_CHAR",
      "position" : 8
    },
    {
      "token" : "国歌",
      "start_offset" : 7,
      "end_offset" : 9,
      "type" : "CN_WORD",
      "position" : 9
    }
  ]
}

```



create a mapping

```json
curl -XPOST http://localhost:9200/index/_mapping -H 'Content-Type:application/json' -d'
{
        "properties": {
            "content": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_smart"
            }
        }
}
```



Dictionary Configuration

```shell
cd elasticsearch-7.1.0/config/analysis-ik
vi IKAnalyzer.cfg.xml
```



IKAnalyzer.cfg.xml 内容

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
	<comment>IK Analyzer 扩展配置</comment>
	<!--用户可以在这里配置自己的扩展字典 -->
	<entry key="ext_dict">custom/mydict.dic;custom/single_word_low_freq.dic</entry>
	 <!--用户可以在这里配置自己的扩展停止词字典-->
	<entry key="ext_stopwords">custom/ext_stopword.dic</entry>
 	<!--用户可以在这里配置远程扩展字典 -->
	<entry key="remote_ext_dict">location</entry>
 	<!--用户可以在这里配置远程扩展停止词字典-->
	<entry key="remote_ext_stopwords">http://xxx.com/xxx.dic</entry>
</properties>
```



热更新 IK 分词

```xml
	<!--用户可以在这里配置远程扩展字典 -->
	<entry key="remote_ext_dict">http://127.0.0.1/my_extra_word.dic</entry>
	<!--用户可以在这里配置远程扩展停止词字典-->
	<!-- <entry key="remote_ext_stopwords">words_location</entry> -->
```

http://127.0.0.1/my_extra_word.dic 满足一下要求

1. 该 http 请求需要返回两个头部(header)，一个是 `Last-Modified`，一个是 `ETag`，这两者都是字符串类型，只要有一个发生变化，该插件就会去抓取新的分词进而更新词库。
2. 该 http 请求返回的内容格式是一行一个分词，换行符用 `\n` 即可。

可以将需自动更新的热词放在一个 UTF-8 编码的 .txt 文件里，放在 nginx 或其他简易 http server 下，当 .txt 文件修改时，http server 会在客户端请求该文件时自动返回相应的 Last-Modified 和 ETag。可以另外做一个工具来从业务系统提取相关词汇，并更新这个 .txt 文件。



将 my_extra_word.dic 放到 httpd 服务器上，没有问题。



==但是自己写http 服务，有问题(目前未解决)==：

```java

    @RequestMapping("/getCustomDict.dic")
    public void getCustomDict(HttpServletRequest request, HttpServletResponse response) {
        try {
            // 读取字典文件
            String path = "/Users/dongyifeng/soft/elasticsearch-7.1.0/config/analysis-ik/my_extra_word.dic";
            File file = new File(path);
            String content = "";
            if (file.exists()) {
                // 读取文件内容
                FileInputStream fi = new FileInputStream(file);
                byte[] buffer = new byte[(int) file.length()];
                int offset = 0, numRead = 0;
                while (offset < buffer.length && (numRead = fi.read(buffer, offset, buffer.length - offset)) >= 0) {
                    offset += numRead;
                }
                fi.close();
                content = new String(buffer, "UTF-8");
            }
            // 返回数据
            OutputStream out = response.getOutputStream();
            response.setHeader("Last-Modified", String.valueOf(content.length()));
            response.setHeader("ETag", String.valueOf(content.length()));
            response.setContentType("text/plain; charset=UTF-8");
            out.write(content.getBytes("UTF-8"));
            response.setCharacterEncoding("UTF-8");
            out.flush();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
```

