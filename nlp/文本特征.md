[TOC]

# 词频统计

## nltk 词频统计

```python
import nltk
from nltk.book import *
fdist1 = FreqDist(["a","b","a"])
# FreqDist({'a': 2, 'b': 1})
# 遍历词频
for key in fdist1:
  print(key,fdist1[key])

# 词频 top 10
fdist1.most_common(10)

# 最高词频
fdist1.max()

# 低频列表
fdist1.hapaxes()
```

词频可视化：

```python
# 词频 top 10 的折线图 
fdist1.plot(10)

# 表格打印词频 top 10
fdist1.tabulate(10)
# a b
# 2 1
```



## python 自带 collections

```python
import collections

collections.Counter(['a', 'b', 'c', 'a', 'b', 'b'])
collections.Counter({'a':2, 'b':3, 'c':1})
collections.Counter(a=2, b=3, c=1)
# 字符统计
counter = collections.Counter('abcdaab')

# 词频 top 10
counter.most_common(10)
```

```python
import collections

c1 = collections.Counter(['a', 'b', 'c', 'a', 'b', 'b'])
c2 = collections.Counter('alphabet')

print 'C1:', c1
print 'C2:', c2

print '\nCombined counts:'
print c1 + c2

print '\nSubtraction:'
print c1 - c2

print '\nIntersection (taking positive minimums):'
print c1 & c2

print '\nUnion (taking maximums):'
print c1 | c2
```

## sklearn 实现







# 条件频率分布

```python
import nltk
from nltk.corpus import brown
# 与词频统计比，每个词都有一个类型（条件，事件）
pairs = [(genre, word) for genre in brown.categories() for word in brown.words(categories=genre)]
cfd = nltk.ConditionalFreqDist(pairs)

# 条件列表
print(cfd.conditions())


genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']
modals = ['can', 'could', 'may', 'might', 'must', 'will']

# 表格打印
cfd.tabulate(conditions=genres, samples=modals)
# 折线图
cfd.plot(conditions=genres, samples=modals)

```

```shell
                  can could   may might  must  will 

           news    93    86    66    38    50   389 

       religion    82    59    78    12    54    71 

        hobbies   268    58   131    22    83   264 

science_fiction    16    49     4    12     8    16 

        romance    74   193    11    51    45    43 

          humor    16    30     8     8     9    13
```



# Text 对象

```python
import nltk
from nltk.book import *

# 字符串查询
text1.concordance("monstrous")

# 与 monstrous 相关的词
text1.similar('monstrous')

# 打印出列表中所有单词共同的上下文
text2.common_contexts(["monstrous", 'very'])

# 自动生成文章
text3.generate()

# 判断重复词密度
len(text3) / len(set(text3))

# 判断关键词密度
text3.count('smote') / len(text3)

# 文本中频繁出现的双连词
text1.collocations()
```





Text 可视化

```python
# 每个单词在文本中的分布情况
text4.dispersion_plot(['freedom', 'America'])
```

