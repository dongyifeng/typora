[TOC]

朴素贝叶斯（Naive Bayes）

# 贝叶斯原理

贝叶斯公式：$P(y_i|x) = \frac{p(y_i) \;*\; p(x|y_i)}{p(x)}=\frac{p(y_i) \;*\; p(x_1,x_2,...,x_n|y_i)}{p(x)}$

假设特征独立（之所以朴素的原因）

$p(x_1,x_2,...,x_n|y_i)=\prod_j{p(x_j|y_i)}$

$P(y_i|x) = \frac{p(y_i) \;*\; p(x|y_i)}{p(x)}=\frac{p(y_i) \;*\; p(x_1,x_2,...,x_n|y_i)}{p(x)}=\frac{p(y_i) \;*\; \prod_j{p(x_j|y_i)}}{p(x)}$



由于 $p(x_j|y_i) \in[0,1] $ 连续乘积造成小数溢出。可以转换为 log 加法。

 $P(y_i|x) \propto \frac{p(y_i) \;*\; \sum_j{log(p(x_j|y_i))}}{p(x)} $

对于分类问题，x 是属于$y_1,y_2,...,y_k$ 中哪一类，p(x) 都是常量，可以省略，对分类结果没有影响。

 朴素贝叶斯最终模型 $P(y_i|x) \propto p(y_i) \;*\; \sum_j{log(p(x_j|y_i))} $ 

# 朴素贝叶斯

## 模型

$P(y_i|x) \propto p(y_i) \;*\; \sum_j{log(p(x_j|y_i))} $ 

- $p(y_i|x)$：x 属于 $y_i$ 分类的概率。
- $p(y_i)$：$y_i$ 类别的先验概率。
- $p(x_j|y_i)$：$y_i$ 类别产生 $x_i$ 特征的概率。

## 策略

极大似然估计（maximum likelihood estimation，MLE）

$p(y_i) = \frac{Count(y_i)}{\sum_j{Count(y_j)}}$

- $Count(y_i)$：类别为 $y_i$ 的对象，在训练数据中出现的次数。
- $ \sum_j{Count(y_j)} $ ：训练集样本数。

$P(x_i|y_j)=\frac{Count(x_i,y_j)}{Count(y_j)}$

- $Count(x_i,y_j)$：特征 $ x_i$ 和类别 $y_j$ 在训练集中同时出现的次数。
- Count(y_j)：训练集中属于类别 $y_j$ 的样本数。



例如：

​		训练集共有 1000 篇新闻，其中军事类：300 篇，科技类：240篇，生活类：140篇，...

​		在军事新闻中：“谷歌”  出现15篇，“投资” 出现 9 篇，“上涨” 出现 36 篇...

$p(军事) = \frac{300}{1000}=0.3\;；p(科技)=\frac{240}{1000}=0.24\;；p(生活)=\frac{140}{1000}=0.14$

$p(谷歌|军事)=\frac{15}{300}=0.05\; ;p(投资|军事)=\frac{9}{300}=0.03\; ; p(上涨|军事)=\frac{36}{300}=0.12$



## 算法

遍历一遍训练集：计算出：$p(y_i)\;,p(x_j|y_i)$

## 优劣点

### 优点

- 简单有效
- 适用于二分类和多分类问题，分类结果是概率（属于软分类）
- 快速高效
- 方便分布式
- 适用于训练集非常大时。

### 缺点

- 特征独立性假设，有时不合理。

比如：

爸爸打儿子。

儿子打爸爸。

对于朴素贝叶斯来说是一样的。



# 数据稀疏性

$P(y_i|x) \propto p(y_i) \;*\; \sum_j{log(p(x_j|y_i))} $

在 预测时