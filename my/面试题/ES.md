---
typora-root-url: ../../../typora
---

[TOC]

# ES 的理解？

1. ES 是建立在全文搜索引擎库 Lucene 基础之上的一个开源的搜索引擎。 
2. ES 是分布式存储的，并且检索速度快。因此 ES 经常被用在网站搜索，公司内部日志的收集和检索。去快速查询服务器日志和记录，去定位问题。

ES 支持：PB 级别数据的秒查。

特点：

- 分布式、高性能、高可用、易扩展、易维护

使用场景：

- 海量数据的全文检索：搜索引擎、垂直搜索、站内搜索
- 数据分析和聚合
- 日志系统
- 特征存储



# Mapping 是什么？你知道 ES 哪些数据类型

ES 中的 Mapping 优点类似数据库中 的“表结构”。Mapping 是定义索引的结构，包含字段名称、字段类型、字段使用的分词器、是否评分、是否创建索引等等。 

**常见数据类型：**

- String 类型
  - text：可分词
  - keyword：不可分词，数据会作为完整字段进行匹配
- Numerica：数值型
  - 基本数据类型：long、integer、short、byte、double、float、half_float
  - 浮点的高精度类型：sacled_float
- Date：日期类型
- Array：数组类型
- Object：对象
- 二进制类型
- 范围类型
- 嵌套类型
- 地理类型
- IP 类型
- Join 类型
- ...



# 倒排索引是什么？

>  面试官：想了解你对基础概念的认知。

传统的我们的检索是通过文章，逐个遍历找到对应关键词的位置。

而倒排索引，是通过分词策略，形成了<font color=red>词和文章ID的映射关系表</font>，这种词典+映射表即为倒排索引。有了倒排索引，就能实现 O（1）时间复杂度的效率检索文章了，极大的提高了检索效率。



倒排索引的结构包含：

- 词项索引（term index）：<font color=red>存放在 .tip 文件中。</font>存放 FST 的公共前缀和公共后缀。
- 词项字典：存储的关键词，使用 FST（Finite State Transducer） 数据结构压缩。<font color=red>存放在 .tim 文件中。</font>后缀词块，倒排表的指针。
- 倒排表：存储的是：包含当前词项的文档 ID 有序列表，是 int 型。int 的最大值是 2^32 ，所以分片存储的文档是有上限的。<font color=red>存放在 .doc 文件中。</font>



FST 可以理解为前缀树的变种，FST 不但利用公共前缀，还是利用了公共后缀。FST 的对空间的空间的压缩还大。

Lucene4+ 开始大量使用数据结构：FST（Finite State Transducer）

FST 优点：

1. 空间占用小，通过对字典中单词前缀和后缀的重复利用，压缩存储空间。
2. 查询速度快。时间复杂度： O(len(str))



Posting list 使用的压缩算法：（高效压缩、快速的编码解码）

- FOR（Frame Of Reference）：针对<font color=red>稠密</font>数组压缩 

  - 核心思想：用<font color=red>减法</font>来削减数值大小，从而达到降低空间存储。

-  RBM（Roaring Bit Map）：针对<font color=red>稀疏</font>数组压缩

  - 解决 FOR 算法缺陷：如果减法的结果差值，依然很大。
  - 核心思想：通过<font color=red>除法</font>来缩减数值大小，但是并不是直接的相除。将一个数拆成高 16 位和低 16 位。

  

| term index | term dictionary(词项字段) | Posting list（倒排表） | 标记匹配 |
| ---------- | ------------------------- | ---------------------- | -------- |
|            | 小米                      | 1,2,4                  |          |
|            | 手机                      | 1,2,3                  |          |
|            | NFC                       | 4,5                    |          |

<img src="/images/tmp/WX20230315-182834.png" style="zoom:50%;" />

<img src="/images/tmp/WX20230307-163227@2x.png" style="zoom:30%;" />



#  倒排索引压缩算法

压缩倒排表目的：

- 节省空间：
- 加速性能：空间占用小了，磁盘 IO 就小。

## FOR



<img src="/images/tmp/WX20230315-184413.png" style="zoom:50%;" />





## RBM

<img src="/images/tmp/WX20230306-220440@2x.png" style="zoom:30%;" />

Short[] 数组存放高位，相同高位存放 value。

value 的类型：

1. ArrayContainer：当 value 小于2096 个数或者 8KB时，使用 ArrayContainer。否则使用 BitMapContainer。

2. BitMapContainer：

3. RunContainer

   <img src="/images/tmp/WX20230306-220917@2x.png" style="zoom:30%;" />







# 分片数

当索引非常大时，一台机器可能放不下或者为了数据安全考虑，需要切分成多份，存储在不同的机器上。每一分数据就是一个分片。ES 的每个分片就是一个 Lucene 的实例，具有完备的全文检索的功能。

分片一般分为主分片和副本分片。

主分片：负责读写功能

副本分片：是只读的，数据从主分片同步过来。



分片和副本的设计为 ES 提供了支持<font color=red>分布式</font>和<font color=red>故障转移</font>的特性。

一个分片就是一个独立 lucene 索引。如果多个分片在同一个节点上，这些分片之间就要竞争相同的资源了。

计算相关度的词频统计信息是基于分片的，如果分片过多，每个分片只有很少的数据会导致很低的相关度。

设置分片数，需要开发人员对业务增长有个预判。



## 分片作用

- <font color=red>实现高可用</font>：分片均衡地分布在不同节点上，能够降低局部故障对整体的影响。
- <font color=red>提高吞吐能力</font>：多个分片往往需要多个主机承载，机器多了吞吐量有所提高。
- 提高<font color=red>并发响应</font>能力：在 ES  中，只有主分片是可读可写的，副本是只读的。通过增加副本数量，可以提高服务的<font color=red>并发查询的能力。</font>



## 分片策略

每个分片就是一个 Lucene 的实例，具有完备的全文检索的功能。

- 分片创建策略：在创建分片之初，指定分配的数量和大小。
- 分片的分配策略：分片会尽可能的分配在不同机器上。
- 分片再均衡策略：有新的节点上线或者有旧节点下线时，ES 会自动重新分配分配，使每个节点上分片比较均衡。
- 分片的延迟分配策略：比如有个节点丢失，ES  会延迟一分钟去执行再分配策略。避免偶尔网络闪断，导致 ES 去执行再平衡。ES 再平衡时，需要消耗大量的网络资源。影响线上服务的稳定。
- 分片的数量决策：没有绝对的最有值的。避免分片过多
- 分片大小的决策：没有绝对的最有值的。（业务）10G ~ 32 G，日志类： 50G





遵循原则：

- 搜索类：一个分片不超过 32 G；日志类：单个分片不超过 50G。
  - 如果索引总容量是 500G，分片大小在 16 个左右。

- 设置分片数一般不超过节点数的 3 倍
  - 如果节点上上有过多的分片，一旦发生节点故障，有可能会导致数据丢失，集群无法恢复。

- 节点数 <= 主分片数 * （副本数 + 1）

 

两个典型的场景：

- 搜索：数据增长较慢。
- 日志：增长快，需要冷热分离，需要自动删除。



默认情况下副本数=1



最理想的分片数量应该依赖于节点的数量。

分配副本分片数的公式：max（max_failures，ceil（num_nodes / num_primaries） -  1）。

- num_nodes：集群节点数
- num_primaries：主分片数
- max_failures：希望最多能够同时处理 max_failures 节点故障
- ceil：向上取整。



# 选主流程

ES 7.x 之<font color=red>前</font>采用的是 <font color=red>bully</font> 算法进行选择。

ES 7.x 之<font color=red>后</font>采用的是 <font color=red>raft</font> 算法进行选择。



## Bully 算法

bully 算法：每个节点都有一个编号，只有编号最大的存活节点才能成为master节点。

选举流程：

1. 每个节点计算最高的已知节点ID，并向该节点发送领导投票
2. 如果节点 node 得不到任何回复(回复为 alive 消息)，那么节点 node 成为 master ，并向所有的其它节点宣布自己是 master (宣布为Victory消息)
3. 如果 node 得到了任何回复，node 节点就一定不是 master，同时等待 Victory 消息，如果等待 Victory 超时那么重新发起选举



Elasticsearch 编号比较的判断依据有两个，首先是 ClusterState版本号的比较，版本号越大优先级越高，然后是节点id的比较，id越小优先级越高。ClusterState是Master向集群中各个节点发送的集群状态，这个状态有一个版本号码，如果集群状态发生了变化，比如集群新增了节点成员或者有节点成员退出了，那么这个版本号就会加一，比对这个版本号的目的是让拥有最新状态的节点成为Master的优先级最高。



bully Master 假死

Master 节点承担的职责负载过重的情况下，可能无法即时对组内成员作出响应，这种便是<font color=red>假死</font>。如果上图中的P6节点假死，于是 P5节点成为了Master节点，但是在 P6 节点负载减轻之后，P6节点又对组内成员作出了响应，P6节点又会成为Master节点，如此反复，整个集群状态就会非常不可靠。

Elasticsearch是如何解决这个问题的呢？在Elasticsearch中，P3节点发现Master P6对自己长时间不作出响应，P3 节点会<font color=red>请求其它节点</font>判断 P6节点是否存活，如果<font color=red>有1/2以上节点都认定P6存活</font>，那么P3就会放弃发起选举



## Raft 算法

7.X之后的ES，采用一种新的选主算法，实际上是 Raft 的实现，但并非严格按照 Raft 论文实现，而是做了一些调整。



Raft 算法

在 Raft 中，节点可能的状态有三种

- 领导者（Leader）：霸道总裁，一切以我为准。处理写请求、管理日志复制和<font color=green>不断地发送心跳信息</font>，通知其他节点“我是领导者，我还活着，你么不要发起新的选举”。
- 候选人（Candidate）：候选人将向其他节点请求投票 RPC 消息，<font color=green>通知其他节点来投票</font>，如果赢得大多数选票，就晋升当领导。
- 跟随者（Follower）：普通群众，默默接收来自领导的消息，当领导者心跳信息超时的时候，就主动站出来，<font color=green>推荐自己当候选人。</font>



Raft 算法通过以下几个关键机制，保证了一个任期只有一位领导者，极大减少了选举失败的情况。

- 引入任期机制：自增，低任期作废。
- 先来先服务的投票原则：同一任期只投票，过期任期拒绝投票。
- <font color=red>随机选举超时时间</font>
- 大多数选票原则



**选举流程**

- 当 A 节点的超时时间到了后，没有收到主节点的心跳，A 节点成为候选者，并增加自己的任期编号，Term 值从 0 更新为 1，并给自己投了一票。
- 节点 A 成为候选者后，向其他节点发送投票请求 RPC 信息，请他们选举自己为领导者。
- 节点 B 和 节点 C 接收到节点 A 发送的请求投票信息后，在编号为 1 的任期内，节点 B 和 节点 C 还没有进行过投票，就把选票投给节点 A，并增加自己的任期编号。<font color=green>每个节点在一个任期内只能投一票。</font>
- 节点 A 收到 3 次投票（包括自己的一票），得到了大多数节点$\frac{n}{2}+1$ 的投票，从候选者成为本届任期内的新的领导者。
- 节点 A 作为领导者，固定的时间间隔给节点 B 和节点 C 发送心跳信息，告诉节点 B 和节点 C ，我是领导者。
- 节点 B 和节点 C 发送相应信息给节点 A ，告诉节点 A 我是正常的。



**如果发生选票瓜分：**
如果有多个 Follower 同时成为 Candidate，那么选票可能会被瓜分以至于没有 Candidate 可以赢得半数以上的投票。当这种情况发生的时候，每一个 Candidate 都会竞选超时，然后通过增加当前 Term 号来开始一轮新的选举。
在选票瓜分的情况下：每一个 Candidate 在开始一次选举的时候会<font color=red>**重置一个随机的选举超时时间**</font>，然后在超时时间内等待投票的结果；这样减少了在新的选举中另外的选票瓜分的可能性。



- **恢复为跟随者状态**：如果一个候选人或者领导者，发现自己的任期编号比其他节点小（改朝换代了），那么它会立即恢复成跟随者状态。比如领导者任期为 3，由于自己不健康，产生了新的领导者，那么前者将立即恢复成跟随者。
- **拒绝消息：**如果一个节点接收到较小的任期编号的请求，那么它会直接拒绝这个请求。可能这个请求是已经被废弃的领导者发过来的。



ES 中的 Raft 算法改动点

1. ES 实现中，候选人不先投自己，而是直接并行发起 RequestVote（选举请求）。这样的好处是可以在一定程度上避免 3 个节点同时成为候选人时，都投自己，无法成功选主的情况。
2. ES 不限制每个节点在某个 term 上只能投一票，节点可以投多票。
   1. 这样会产生选出多个主的情况，ES 的处理是让最后当选的 Leader 成功。



# 脑裂

ES在主节点上产生分歧，产生多个主节点，从而使集群分裂，使得集群处于异常状态。这个现象叫做脑裂



解决思路：

1. 网络问题：保证网络稳定，及时预警，重启集群

2. master节点负载过大：

   1. 避免master节点因为工作负载过大出现响应中断从而引发脑裂。master节点专门做集群master管理，master节点配置

      ```shell
      node.master: true
      node.data: false
      ```

      同时设置一批data节点负责存储数据和处理请求

      ```shell
      node.master: false 
      node.data: true
      ```

      如果确实还是顶不住，那么就可以再设置一批client节点只负责处理用户请求，实现请求转发，负载均衡等功能，让data节点只负责存储数据

      ```shell
      node.master: false  
      node.data: false
      ```

      

3. 优化方法

   1. **discovery.zen.minimum_master_nodes**
      1. (N/2)+1：选举过半，其中N是具有master资格的节点的数量，设置这个参数后，只有足够的 master 候选节点时，才可以选举出一个master
   2. **discovery.zen.ping_timeout**
      1. 节点等待响应的时间，默认值是3秒，增加这个值，会增加节点等待响应的时间，从一定程度上会减少误判

**脑裂修复**

当脑裂发生后，唯一的修复办法是解决这个问题并<font color=red>重启集群</font>。

当elasticsearch集群启动时，会选出一个主节点（一般是启动的第一个节点被选为主）。由于索引的两份拷贝已经不一样了，elasticsearch会认为选出来的主保留的分片是“主拷贝”并将这份拷贝推送给集群中的其他节点。如果主节点不是最新的数据，会导致丢失数据。



第一个建议是给所有数据重新索引。

```json
// 数据迁移
POST _reindex{  
"source": {    "index": "twitter"  }, 
 "dest": {    "index": "new_twitter",   
 "op_type": "create"  }
}
```



第二，逐个关闭节点并备份数据，分析比对数据是否是最新的。如果是保存的数据是最新的，启动它并且让它被选为主节点。然后就可以启动集群的其他节点了



# 主分片数量可以在后期更改吗？为什么？

不可以，因为根据路由算法shard = hash(document_id) % (num_of_primary_shards)，当主分片数量变化时会影响数据被路由到哪个分片上。



# 详细描述一下 Elasticsearch 索引文档的过程。

协调节点默认使用文档 ID 参与计算（也支持通过 routing），以便为路由提供合适的分片。
shard = hash(document_id) % (num_of_primary_shards)

1. 当分片所在的节点接收到来自协调节点的请求后，会将请求写入到 MemoryBuffer，然后定时（默认是每隔 1 秒或者 buffer 写满，默认是堆内存的 10%，最小 48M）写入到 Filesystem Cache，这个从 MomeryBuffer 到 Filesystem Cache 的过程就叫做 refresh（refresh 是在  JVM 中，消耗 JVM 性能，从性能上来说，越少越好）；
2. 当然在某些情况下，存在 Momery Buffer 和 Filesystem Cache 的数据可能会丢失，ES 是通过 translog 的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到 translog 中 ，当 Filesystem cache 中的数据写入到磁盘中时，才会清除掉，这个过程叫做 flush；
3. 在 flush 过程中，内存中的缓冲将被清除，内容被写入一个新段（段合并），段的 fsync将创建一个新的提交点，并将内容刷新到磁盘，旧的 translog 将被删除并开始一个新的 translog。
4. flush 触发的时机是定时触发（默认 30 分钟）或者 translog 变得太大（默认为 512M）时；

<img src="/images/es/WX20230302-113652@2x.png" style="zoom:33%;" />





# 写优化

针对搜索要求不高，但是对写入要求较高的场景（日志收集）

设计阶段：

- SDD 硬盘、64G 内存，CPU 多核
- 优化节点间的任务分布，尽量在同一个机房中。
- 将一半的内存交给 Lucene（ES 的 JVM 不要超过 32 G）
- 合理分片数和副本数。
- 通用最小化算法，能用更小的字段类型就用更小的，keyword 类型比 int 更快。
- Ignore_above：字段保留的长度，越小越好。（ 太长的字段存入 ES，会根据 ignore_above 的值，进行截断 ）
- 禁用 all 字段：all 字段包含所有字段分词后的 Term，作用是可以在搜索时不指定特定字段，从所有字段中检索，ES 6.0 之前需要手动关闭。
- 关闭 Norms 字段：计算评分用的，如果确定当前字段不需要评分，设置 false 可以节省大量的磁盘空间，有助于提升性能。常见的比如 filter 和 agg 字段，都可以设为关闭。
- 冷热分离的架构设计



执行阶段：

- 如果不考虑搜索结果的实行性要求不高：
  - 加大 Translog Flush 的时间间隔，目的是降低 Iops、Writeblock
  - 增加 Index Refresh 间隔，目的是减少 Segment Merge 的次数
  - 增加 index Buffer 大小，本质也是减少 refresh 的时间间隔，因为导致 segment 文件创建的原因不仅有时间间隔，还有 buffer 空间的大小，写满了也会创建。默认最小值 48M < 堆空间的 10%（默认）< 默认最大限制

- 大批量的数据写入时，尽量控制在低检索请求的时间段，大批量的写入请求越集中越好。
  - 第一是减小读写之间的资源抢占，读写分离
  - 当检索请求数量很少时，可以减少甚至完全删除副本分片，关闭 segment 的自动创建以达到高效利用内存的目的，因为副本的存在会导致主从之间频繁的数据同步，大大增加服务器资源的占用。
- 调整 Bulk 线程池和队列，每次批量数据 5–15 MB 大是个不错的起始点。
- 调整 _source 字段，通过 include 和 exclude 过滤，查询的字段越少越好，过滤不必要的字段。
- Lucene 的数据的 fsync 是发生在 OS cache ，所以要给 OS cache 预留足够的内存大小。
- store：开辟另一块存储空间，可以节省带宽。 
- 关闭 index_options（谨慎使用，高端操作）：词设置用于在 index_time 过程中，哪些内容会被添加到倒排索引的文件中，例如：TF、docCount、position、offsets等，减少 option 的选择可以减少在创建索引时的 CPU 占用率，不过在实际场景中很难确定业务是否会用到这些信息，除非是一开始就非常确定用不到，否则不建议删除。
- 优化 Lucene 层的索引建立，目的是降低 CPU 及 IO



ES 提供了 Bulk API 支持批量操作，大量写入任务时，使用 Bulk 来进行批量写入。Bulk 默认设置批量提交的数据量不能超过 100M。单次批量处理的数据大小应从 5MB ~ 15MB 逐渐增加，当性能没有提升时，把这个数据量作为本系统的最大值。



Lucene 在数据新增时，采用了延迟写入的策略，默认情况下，索引的 refresh_interval 为 1 秒。

Lucene 将待写入的数据写到内存中，超过 1 秒就会触发一次 Refresh，然后 Refresh 会把内存中的数据刷新到操作系统文件缓存系统中。

减少副本数量。



SSD 硬盘。



补充：关于 Lucene 的 Segement：

1. Lucene 索引是由多个段组成，段本身是一个功能齐全的倒排索引。
2. 段是不可变的，允许 Lucene 将新的文档增量地添加到索引中，而不用从头重建索引。
3. 对于每一个搜索请求而言，索引中的所有段都会被搜索，并且每个段会消耗CPU 的时钟周、文件句柄和内存。这意味着段的数量越多，搜索性能会越低。
4. 为了解决这个问题，Elasticsearch 会合并小段到一个较大的段，提交新的合并段到磁盘，并删除那些旧的小段。



# 搜索速度优化

设计阶段：

- 利用自适应副本选择（ARS）提升 ES 响应速度。
- SDD 硬盘、64G 内存，CPU 多核
- 优化节点间的任务分布，尽量在同一个机房中。
- 将一半的内存交给 Lucene（ES 的 JVM 不要超过 32 G）
- 合理分片数和副本数。
- 通用最小化算法，能用更小的字段类型就用更小的，keyword 类型比 int 更快。
- Ignore_above：字段保留的长度，越小越好。（ 太长的字段存入 ES，会根据 ignore_above 的值，进行截断 ）
- 禁用 all 字段：all 字段包含所有字段分词后的 Term，作用是可以在搜索时不指定特定字段，从所有字段中检索，ES 6.0 之前需要手动关闭。
- 关闭 Norms 字段：计算评分用的，如果确定当前字段不需要评分，设置 false 可以节省大量的磁盘空间，有助于提升性能。常见的比如 filter 和 agg 字段，都可以设为关闭。
- 冷热分离的架构设计



- 数据预热
- 冷热分离的架构设计
- Document 设计：
  - 通用最小化算法，能用更小的字段类型就用更小的，keyword 类型比 int 更快。
  - 禁用 all 字段：all 字段包含所有字段分词后的 Term，作用是可以在搜索时不指定特定字段，从所有字段中检索，ES 6.0 之前需要手动关闭
  - 某些字段关闭正排索引：文章正文。
  - 某些字段关闭倒排索引：仅仅是粗排时使用，不会被检索。



- 使用 filter 代替 query：filter 不计算相关度评分。

- 避免深度分页，避免分片数量过多，es 提供两种解决方案：scroll search 和 search after

  



# 详细描述删除和更新文档的过程

1. 删除和更新也都是写操作，但是 Elasticsearch 中的文档是不可变的，因此不能被删除或者改动以展示其变更；
2. 磁盘上的每个段都有一个相应的.del 文件。当删除请求发送后，文档并没有真的被删除，而是在.del 文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在.del 文件中被标记为删除的文档将不会被写入新段。
3. 在新的文档被创建时，Elasticsearch 会为该文档指定一个版本号，当执行更新时，旧版本的文档在.del 文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。

ES 是逻辑删除，将删除的数据存放在 .del 文件中。该文档依然能匹配查询，但是会在结果中被过滤掉。当段段合并时，.del 文件的数据不会被写入新段。



# 详细描述一下 Elasticsearch 搜索的过程。

1. 搜索被执行成一个两阶段过程，我们称之为 Query Then Fetch；
2. 在初始查询阶段时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。
3. 每个分片返回各自优先队列中 所有文档的 ID 和排序值 给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。
4. 接下来就是 取回阶段，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并 丰 富 文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。
5. 补充：Query Then Fetch 的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，DFS Query Then Fetch 增加了一个预查询的处理，询问 Term 和 Document frequency，这个评分更准确，但是性能会变差。*





# 在 Elasticsearch 中，是怎么根据一个词找到对应的倒排索引的？

1. Lucene 的索引过程，就是按照全文检索的基本过程，将倒排表写成此文件格式的过程。
2. Lucene 的搜索过程，就是按照此文件格式将索引进去的信息读出来，然后计算每篇文档打分(score)的过程。





# 对于 GC 方面，在使用 Elasticsearch 时要注意什么？

1. 倒排词典的索引需要常驻内存，无法 GC，需要监控 data node 上 segmentmemory 增长趋势。
2. 各类缓存，field cache, filter cache, indexing cache, bulk queue 等等，要设置合理的大小，并且要应该根据最坏的情况来看 heap 是否够用，也就是各类缓存全部占满的时候，还有 heap 空间可以分配给其他任务吗？避免采用 clear cache等“自欺欺人”的方式来释放内存。
3. 避免返回大量结果集的搜索与聚合。确实需要大量拉取数据的场景，可以采用scan & scroll api 来实现。
4. cluster stats 驻留内存并无法水平扩展，超大规模集群可以考虑分拆成多个集群通过 tribe node 连接。
5. 想知道 heap 够不够，必须结合实际应用场景，并对集群的 heap 使用情况做持续的监控。
6. 根据监控数据理解内存需求，合理配置各类circuit breaker，将内存溢出风险降低到最低。





# 在并发情况下，Elasticsearch 如果保证读写一致？

1. 可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；
2. 另外对于写操作，一致性级别支持 quorum/one/all，默认为 quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。
3. 对于读操作，可以设置 replication 为 sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置 replication 为 async 时，也可以通过设置搜索请求参数_preference 为 primary 来查询主分片，确保文档是最新版本。



# 如何监控 Elasticsearch 集群状态？

Marvel 让你可以很简单的通过 Kibana 监控 Elasticsearch。你可以实时查看你的集群健康状态和性能，也可以分析过去的集群、索引和节点指标。



# 介绍一下你们的个性化搜索方案？

基于word2vec和Elasticsearch实现个性化搜索

1. 基于word2vec、Elasticsearch和自定义的脚本插件，我们就实现了一个个性化的搜索服务，相对于原有的实现，新版的点击率和转化率都有大幅的提升；
2. 基于word2vec的商品向量还有一个可用之处，就是可以用来实现相似商品的推荐；
3. 使用word2vec来实现个性化搜索或个性化推荐是有一定局限性的，因为它只能处理用户点击历史这样的时序数据，而无法全面的去考虑用户偏好，这个还是有很大的改进和提升的空间；



# ES 的节点类型

- Master：候选节点
- data：数据节点
- voting_only：仅投票节点
- Ingest：预处理节点，作用类似 Logstash 中的 Filter
- ml：机器学习节点
- 
- data_content：数据内容节点
- data_hot：热节点
- data_warm：索引不在定期更新，但认可查询
- data_code：冷节点，只读索引
- Remote_cluster_client：候选客户端节点
- Transform：转换节点



# ES 深度翻页？

对于深度翻页，ES 的每个 shard 都会查询 Top N（N = from + size）的结果，并且每个 shard 会将这 N 条数据传给协调节点，那么协调对 N * 分片数 条数据，进行排序后，获取 【 from, from + size 】 的数据。



解决方案：

- Scroll 滚动游标 方式
  - 原理：是因为如果做深分页，每次搜索都必须重新排序，非常浪费，使用scroll就是一次把要用的数据都排完了（做成快照），分批取出，因此比使用 from + size还好。
  - 优点
    - 支持全量遍历
  - 缺点
    - 响应时间非实时：SearchScroll 由于会 “打<font color=red>snapshot</font>”，context会保留目前的segments，后续写入的数据都是感知不到的
    - 保留上下文需要足够的堆内存空间。保存上下文信息。
  - 场景：
    - 全量或数据量很大时遍历结果数据，而非分页查询。

- Search After
  - 原理：其思想是使用来自前一页的结果来帮助检索下一页。sort 排序值可以被用于 search_after 参数里以便抓取下一页的数据。比如，我们可以使用最后的一个文档的 sort 排序值，将它传递给 search_after 参数。
  - search_after 的后续查询都是基于 PIT 视图进行，能有效保障数据的一致性。与<font color=red>快照类似</font>
  - 优点：
    - 不严格受制于 max_result_window，可以无限制往后翻页。
  - 缺点：
    - 只支持向后翻页，<font color=red>不支持随机翻页</font>。
  - 场景：
    - 不支持随机翻页，更适合手机端应用的场景。 



- 官方文档强调：不再建议使用scroll API进行深度分页。如果要分页检索超过 Top 10,000+ 结果时，推荐使用：PIT + search_after。



# 如何进行中文分词？用过哪些分词器？



常见的中文分词器：

- IK 分词
- HanLP
- 结巴分词
- 清华大学THULAC
- 中科院计算所NLPIR





# 工作中都有哪些场景使用了 ES？

- 搜索业务：
  - 场景：帖子搜索、股票搜索、用户搜索、基金搜索等等。
  - 此场景的特点
    - 高性能：10W QPS，10ms 级别平均响应。
    - 强相关：搜索结果高度匹配用户意图。
    - 高可用：可用性达9999，跨机房容灾。
- 日志实时分析：
  - 场景：系统状态日志：服务质量。业务状态日志：运行分析。用户行为日志：用户画像分析、操作审计。
  - 场景特点：  
    - 时效性：从日志产生到可访问，10秒级或者1分钟
    - 高性能：万亿级日志，秒级响应
    - 灵活性：接口易用灵活，类搜索引擎。
- 特征存储：



1. 





# ES 打分机制

Elasticsearch 5 之前的版本 boost * TF * IDF

- TF( Term Frequency )：搜索词在文本中出现的次数，出现次数越大，文本与Query 越相关。

- IDF（Inverse Document Frequency）：搜索词在整个索引库的所有文档中出现的次数，出现的次数越多，说明这个词越不重要（停用词）。



Elasticsearch 5 之后默认是 BM25（Best Match）：计算 query 与 文档相似度得分的经典算法（Query 短，文档长）

 

## TF-IDF与BM25 的相同点

TF-IDF 和 BM25 同样使用 逆向文档频率 来区分普通词（不重要）和非普通词（重要），同样认为：

- 文档里的某个词出现次数越频繁，文档与这个词就越相关，得分越高。
- 某个词在集合所有文档里出现的频率是多少？频次越高，权重越低，得分越低 。某个词在集合中所有文档中越罕见，得分越高。



## TF-IDF与BM25 的不同点

BM25在传统TF-IDF的基础上增加了几个可调节的参数，使得它在应用上更佳灵活和强大，具有较高的实用性。

- 传统的TF值理论上是可以无限大的。而BM25与之不同，它在TF计算方法中增加了一个常量 k，用来<font color=red>限制 TF 值的增长极限</font>。下面是两者的公式：

  - 传统 TF：$score = \sqrt{tf}$
  - BM25 TF：$score = \frac{(k+1)*tf}{k+tf}$

  

- BM25还引入了<font color=red>平均文档长度</font>的概念，单个文档长度对相关性的影响力与它和平均长度的比值有关系。BM25的TF公式里，除了常量k外，引入另外两个参数：L 和 b。

  - L 是文档长度与平均长度的比值。如果文档长度是平均长度的2倍，则L＝2。
  - b 是一个常数，它的作用是规定 L 对评分的影响有多大。加了 L 和 b 的公式变为：



在 $tf_{td}$ 相同时，文章越短，越相关。

文章长度相同时，$tf_{td}$ 越大，越相关。



# Elasticsearch 哪些查询影响相关性评分？

每个 must，should和must_not元素称为查询子句。

- 文档满足 <font color=red>must或 should</font>条款的标准的程度有<font color=red>助于文档的相关性得分</font>。分数越高，文档就越符合您的搜索条件。
- must_not子句中的条件被视为过滤器。<font color=red>**不会影响文档的评分方式。**</font>
- filter：必须 匹配，但它以**不评分**、过滤模式来进行。filter内部语句对评分没有贡献，只是根据过滤标准来排除或包含文档。

总结：<font color=green>**filter、must_not不影响评分，其他影响评分**。</font>



## 自定义评分

- Boosting 
  - 创建索引时修改文档属性
    - 弊端：**修改 boost 值的唯一方式是重建索引，reindex 数据，成本太高了**！
  - 查询时不同的查询条件设置不同的 boost 值
- boost 取值：0 - 1 之间的值，如：0.2，代表降低评分；
- boost取值：> 1， 如：1.5，代表提升评分。



## function_score 平分









# ES 线上问题排查思路



# ES 运维

## ES 迁移

### 离线迁移

- Reindex ( 5.x ES 接口，同一个集群经常用，慢，需要开启白名单 )
- ElasticSearch-dump（数据量小，类似 mysqldump 这种，需要开启白名单）
- Logstash（有门槛，需要开启白名单）
- Snapshot（数据量大）
  - 第一次全部数据做快照。
  - 之后每次都是增量更新。

### 在线迁移

- 基准
- 增量
- 双写
- 异步
