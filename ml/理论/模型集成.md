[TOC]

# 模型集成

三个臭皮匠顶一个诸葛亮。

![](images/20201103105530.jpg)

将多个模型融合在一起，得到一个单一模型。

应用：分类，回归，聚类，推荐...



## 为什么要集成？

结论：将多个模型融合会得到更好的精确度。

假设有 5 个 Accuracy = 70% 的分类器，<font color=red>相互独立</font>。 采用投票的方式将 5 个分类器的结果进行集成：**当一个样本被三个及以上的分类器判断为正例，那么集成模型就判断为正例**。

集成模型的精度为：$C_5^2*0.7^3*0.3^2+C_5^4*0.7^4*0.3^1+C_5^5*0.7^5=83.7\%$



如果是 101 个分类器（必须保证101个分类器<font color=red>**相互独立**</font>,这个很难），那么 Accuracy = 99.9%



## 模型集成要解决的问题

1. <font color=green>如何获得多个模型，并且尽量相互独立。</font>

2. <font color=green>如何将多模型的结果融合。</font>

   

## 主要的集成思想

1. <font color=green>Committees：委员会，就是投票。每个模型都有投票权。</font>
2. <font color=green>Boosting：贪心算法。如果要训练 n 个模型，每次训练一个模型，根据上一个模型训练的结果训练下一个模型。</font>
3. <font color=green>Space split：将一个很大问题空间分割成 n 块，每次训练一个模型。比如：决策树</font>
4. <font color=green>Mixture Model：空间软化分</font>



# Committees



## 多个模型的结果进行融合。

- 分类问题：投票。
- 回归问题：模型的输出的均值。

## Bagging

解决问题：获得多个模型，并且尽量相互独立。也就是获取多个独立的训练集。可以并行训练模型。

Bagging（Bootstrap Aggregation）自举集成。



算法描述

对训练数据集进行<font color=green>**等概率放回采样**</font>，得到多个训练数据集，分别训练模型。



带放回的采样概率分析：

训练集有 n 条样本，放回的随机抽出 n 个样本。问：每个样本被抽的概率是多少？

分析：

样本被抽到的概率：$\frac{1}{n}$

样本被<font color=red >没</font>抽到的概率：$1-\frac{1}{n}$

n 次抽样都被<font color=red >没</font>抽到的概率：$(1-\frac{1}{n})^n$

n 次抽样都至少一次被抽到的概率：$1-(1-\frac{1}{n})^n$



当 n 很大时：$limit_{n\rightarrow$ }{1-(1-\frac{1}{n})^n}=1-\frac{1}{e}=0.632$

### Bagging 特点

Bagging 适合弱分类器

- 不稳定：随机采样会得到不同的基分类器。
- 每个基分分类器准确率略高于 50%。
- Bagging较适合偏差小，方差大的基分类器（决策树）。

Bagging 不适合强分类器

- 每个基分类器只有更少的训练样本，集成后反而不如单个分类器。

# Boosting

核心思想：对训练数据集进行采样，当前分错的样本采样权重更大。

Boosting 工作机制：

![](images/20201109094047.jpg)

迭代 k 次，得到 k 个模型。

多模型融合：不等权投票，加大误差率小的分类器的投票权重。



## AdaBoost

解决问题：通过不同权重的重采样获取不过训练集。必须顺序训练模型。

### 算法过程

假设是一个二分类任务。

输入：训练集 $T=[(x_1,y_1),(x_2,y_2),...,(x_n,y_n)]\,\,; x_i \in X \subseteq R^n\,;y_i \in Y=[-1,1]$  

输出：G(x)

1. 初始化训练数据的权值分布（等权）：

   $D_1=(w_{1,1},w_{1,2},...,w_{1,i},...,w_{1,n}) \,;w_{1,i}=\frac{1}{N},i = 1,2,...,N$

2. 根据样本的权重，有放回抽样，进行 M 轮，得到 M 个模型。

   1. 在 $D_m$ 上，训练出一个基分类器：$G_m(x):X\rightarrow [-1,+1]$

   2. 计算$G_m(x) $在训练集 $D_m$ 上的分类误差率：$e_m = P(G_m(x_i)!=y_i)=\sum_{i-1}^N{w_{mi}I(G_m(x_i)!=y_i)}$

      <font color=red>注意：加权分类误差率</font> $w_{mi}$

   3. 计算$G_m(x)$ 的系数（模型的权重）：$a_m = \frac{1}{2}log_e{\frac{1-e_m}{e_m}}$

   4. 更新训练数据的权值分布：$D_{m+1}=(w_{m+1,1},w_{m+1,2},...,w_{m+1,i},...,w_{m+1,n})$

      $w_{m+1,i} = \frac{w_{m,i}}{Z_m}e^{-a_my_iG_m(x_i)};i=1,2,...,N$

      $Z_m$规范化因子：$Z_m=\sum_{i=1}^Nexp(-a_my_iG_m(x_i))$

      $Z_m$ 使 $D_m$ 成为一个分布。

3. 构建基本分类器的线性组合：$f(x)=\sum_{i=1}^M{a_mG_m(x)}$

   最终分类器：$G(x)=sign(f(x))=sign(\sum_{i=1}^M{a_mG_m(x_i)})$

```python
def ada_boost_train(data_set, label, num=40):
    class_list = []
    n = len(data_set)
    w = [1.0 / n] * n
    agg_class_est = [0.0] * n

    for i in range(num):
        # 构建基分类器：决策树
        best_stump, error, class_est = build_stump(data_set, label, w)

        alpha = float(0.5 * math.log((1.0 - error) / error, math.e))
        best_stump["alpha"] = alpha
        class_list.append(best_stump)

        print("alpha=%s, classEst=%s, bestStump=%s, error=%s " % (alpha, class_est, best_stump, error))

        expon = [0.0] * n
        for i in range(n):
            expon[i] = -1 * alpha * label[i] * class_est[i]

        for i in range(n):
            w[i] = w[i] * math.e(expon[i])

        z = sum(w)
        w = [w_i / z for w_i in w]

        agg_class_est = [agg_class_est[i] + alpha * class_est[i] for i in range(n)]
    return class_list, agg_class_est
```





